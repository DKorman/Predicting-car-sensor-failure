{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> An approach to predicting car sensor failure </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost\n",
    "from category_encoders.one_hot import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files with the help of a generator -  faster than using for loop\n",
    "sensor_data = glob.glob(\"*.csv\")     \n",
    "df_from_each_file = (pd.read_csv(f) for f in sensor_data)\n",
    "df = pd.concat(df_from_each_file, axis = 0, ignore_index=True)\n",
    "\n",
    "# sort datafame by date for groupby and feature engineering purposes\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df = df.sort_values(by='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10982984, 95)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Z300PMDR           175\n",
       "MK0311YHGHYWMA     175\n",
       "W300AZSC           175\n",
       "Z303ZA70           174\n",
       "JK1105B8GG4X7X     173\n",
       "                  ... \n",
       "W3006E5B             1\n",
       "WD-WCAU45402975      1\n",
       "Z300KHN0             1\n",
       "WD-WCC4N5AHPH31      1\n",
       "Z3015V4D             1\n",
       "Name: serial_number, Length: 697, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print value counts of sensors which had failures\n",
    "failures = df[df['failure'] == 1]\n",
    "serial_numbers = list(failures['serial_number'].unique())\n",
    "subset_of_failures = df[df['serial_number'].isin(serial_numbers)]\n",
    "subset_of_failures['serial_number'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "# number of unique days\n",
    "print(len(df['date'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'serial_number', 'type', 'capacity', 'failure',\n",
      "       'stat_1_normalized', 'stat_1_raw', 'stat_2_normalized', 'stat_2_raw',\n",
      "       'stat_3_normalized', 'stat_3_raw', 'stat_4_normalized', 'stat_4_raw',\n",
      "       'stat_5_normalized', 'stat_5_raw', 'stat_7_normalized', 'stat_7_raw',\n",
      "       'stat_8_normalized', 'stat_8_raw', 'stat_9_normalized', 'stat_9_raw',\n",
      "       'stat_10_normalized', 'stat_10_raw', 'stat_11_normalized',\n",
      "       'stat_11_raw', 'stat_12_normalized', 'stat_12_raw',\n",
      "       'stat_13_normalized', 'stat_13_raw', 'stat_15_normalized',\n",
      "       'stat_15_raw', 'stat_22_normalized', 'stat_22_raw',\n",
      "       'stat_183_normalized', 'stat_183_raw', 'stat_184_normalized',\n",
      "       'stat_184_raw', 'stat_187_normalized', 'stat_187_raw',\n",
      "       'stat_188_normalized', 'stat_188_raw', 'stat_189_normalized',\n",
      "       'stat_189_raw', 'stat_190_normalized', 'stat_190_raw',\n",
      "       'stat_191_normalized', 'stat_191_raw', 'stat_192_normalized',\n",
      "       'stat_192_raw', 'stat_193_normalized', 'stat_193_raw',\n",
      "       'stat_194_normalized', 'stat_194_raw', 'stat_195_normalized',\n",
      "       'stat_195_raw', 'stat_196_normalized', 'stat_196_raw',\n",
      "       'stat_197_normalized', 'stat_197_raw', 'stat_198_normalized',\n",
      "       'stat_198_raw', 'stat_199_normalized', 'stat_199_raw',\n",
      "       'stat_200_normalized', 'stat_200_raw', 'stat_201_normalized',\n",
      "       'stat_201_raw', 'stat_220_normalized', 'stat_220_raw',\n",
      "       'stat_222_normalized', 'stat_222_raw', 'stat_223_normalized',\n",
      "       'stat_223_raw', 'stat_224_normalized', 'stat_224_raw',\n",
      "       'stat_225_normalized', 'stat_225_raw', 'stat_226_normalized',\n",
      "       'stat_226_raw', 'stat_240_normalized', 'stat_240_raw',\n",
      "       'stat_241_normalized', 'stat_241_raw', 'stat_242_normalized',\n",
      "       'stat_242_raw', 'stat_250_normalized', 'stat_250_raw',\n",
      "       'stat_251_normalized', 'stat_251_raw', 'stat_252_normalized',\n",
      "       'stat_252_raw', 'stat_254_normalized', 'stat_254_raw',\n",
      "       'stat_255_normalized', 'stat_255_raw'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10982287\n",
       "1         697\n",
       "Name: failure, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio of positive and negative values in the target variable\n",
    "df['failure'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is heavily imbalanced\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset is heavily imbalanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70923"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of sensors tracked\n",
    "len(df['serial_number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of sensor types\n",
    "len(df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average monthly failure rate is 0.1916313606142306\n"
     ]
    }
   ],
   "source": [
    "# average monthly failure rate \n",
    "df = df.set_index('date')\n",
    "monthly_average_of_failures = df.failure.resample('M').sum()[:5].mean()\n",
    "monthly_average_of_nunique = df.serial_number.resample('M').nunique()[:5].mean()\n",
    "average_failure_rate = (monthly_average_of_failures/monthly_average_of_nunique)*100\n",
    "df.reset_index(inplace=True) \n",
    "print(\"average monthly failure rate is {}\".format(average_failure_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of failures in the following 7 days, for each day in the dataset, is 27.94.\n",
      "In other words, model would need to predict failure in the following 7 days for, in average, 27.94 out of 63350.8 number of monthly active sensors.\n",
      "Tough job!\n"
     ]
    }
   ],
   "source": [
    "# average number of sensor failures in the following 7 days\n",
    "n_failures_in_following_7d = df.groupby('date')['failure'].sum()\n",
    "n_failures_in_following_7d = n_failures_in_following_7d.sort_index(ascending = False )\n",
    "n_failures_in_following_7d = n_failures_in_following_7d.rolling(window = 7, min_periods = 7).sum()\n",
    "n_failures_in_following_7d = n_failures_in_following_7d.sort_index(ascending = True ).mean()\n",
    "print(\"Average number of failures in the following 7 days, for each day in the dataset, is {:.2f}.\".format(n_failures_in_following_7d))\n",
    "print(\"In other words, model would need to predict failure in the following 7 days for, in average, {:.2f} out of {} number of monthly active sensors.\".\\\n",
    "      format(n_failures_in_following_7d, monthly_average_of_nunique))\n",
    "print(\"Tough job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-06-23    4\n",
       "2020-06-22    2\n",
       "2020-06-21    4\n",
       "2020-06-20    0\n",
       "2020-06-19    5\n",
       "             ..\n",
       "2020-01-05    4\n",
       "2020-01-04    3\n",
       "2020-01-03    4\n",
       "2020-01-02    6\n",
       "2020-01-01    4\n",
       "Name: failure, Length: 175, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_failures_in_following_7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2020-01-01    4\n",
       "2020-01-02    6\n",
       "2020-01-03    4\n",
       "2020-01-04    3\n",
       "2020-01-05    4\n",
       "             ..\n",
       "2020-06-19    5\n",
       "2020-06-20    0\n",
       "2020-06-21    4\n",
       "2020-06-22    2\n",
       "2020-06-23    4\n",
       "Name: failure, Length: 175, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_failures_in_following_7d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data wrangling and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best indicators were marked as best in the data science assignment pdf\n"
     ]
    }
   ],
   "source": [
    "# defining variables for future use\n",
    "\n",
    "target_variable = 'failure'\n",
    "\n",
    "numerical_features = ['stat_1_normalized', 'stat_1_raw', 'stat_2_normalized', 'stat_2_raw', 'stat_3_normalized', 'stat_3_raw', 'stat_4_normalized', 'stat_4_raw', 'stat_5_normalized', 'stat_5_raw', 'stat_7_normalized', 'stat_7_raw', 'stat_8_normalized', 'stat_8_raw', 'stat_9_normalized', 'stat_9_raw', 'stat_10_normalized', 'stat_10_raw', 'stat_11_normalized', 'stat_11_raw', 'stat_12_normalized', 'stat_12_raw', 'stat_13_normalized', 'stat_13_raw', 'stat_15_normalized', 'stat_15_raw', 'stat_22_normalized', 'stat_22_raw', 'stat_183_normalized', 'stat_183_raw', 'stat_184_normalized', 'stat_184_raw', 'stat_187_normalized', 'stat_187_raw', 'stat_188_normalized', 'stat_188_raw', 'stat_189_normalized', 'stat_189_raw', 'stat_190_normalized', 'stat_190_raw', 'stat_191_normalized', 'stat_191_raw', 'stat_192_normalized', 'stat_192_raw', 'stat_193_normalized', 'stat_193_raw', 'stat_194_normalized', 'stat_194_raw', 'stat_195_normalized', 'stat_195_raw', 'stat_196_normalized', 'stat_196_raw', 'stat_197_normalized', 'stat_197_raw', 'stat_198_normalized', 'stat_198_raw', 'stat_199_normalized', 'stat_199_raw', 'stat_200_normalized', 'stat_200_raw', 'stat_201_normalized', 'stat_201_raw', 'stat_220_normalized', 'stat_220_raw', 'stat_222_normalized', 'stat_222_raw', 'stat_223_normalized', 'stat_223_raw', 'stat_224_normalized', 'stat_224_raw', 'stat_225_normalized', 'stat_225_raw', 'stat_226_normalized', 'stat_226_raw', 'stat_240_normalized', 'stat_240_raw', 'stat_241_normalized', 'stat_241_raw', 'stat_242_normalized', 'stat_242_raw', 'stat_250_normalized', 'stat_250_raw', 'stat_251_normalized', 'stat_251_raw', 'stat_252_normalized', 'stat_252_raw', 'stat_254_normalized', 'stat_254_raw', 'stat_255_normalized', 'stat_255_raw' ]\n",
    "\n",
    "numerical_features_normalized = [x for x in numerical_features if 'raw' not in x]\n",
    "\n",
    "categorical_features = ['type']\n",
    "\n",
    "best_indicators = ['stat_5_normalized','stat_10_normalized','stat_184_normalized','stat_187_normalized','stat_188_normalized','stat_196_normalized','stat_197_normalized','stat_198_normalized','stat_201_normalized']\n",
    "\n",
    "print(\"best indicators were marked as best in the data science assignment pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting\n",
    "df = df[['date', 'serial_number', 'capacity'] + best_indicators + categorical_features + [target_variable]]\n",
    "# df = df[['date', 'serial_number', 'capacity'] + numerical_features_normalized + categorical_features + [target_variable]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape before semi random subset: (10982984, 14)\n",
      "Dataset shape before after semi random subset: (605636, 14)\n"
     ]
    }
   ],
   "source": [
    "# randomly dropping some sensors which didnt have failure - will solve memory issues due to dataset size/computer capacity, and should not affect the modelling too much because the dataset is heavily imbalanced as is\n",
    "print(\"Dataset shape before semi random subset:\", df.shape)\n",
    "serial_n_to_keep = df['serial_number'].unique()\n",
    "serial_n_to_keep = [x for x in serial_n_to_keep if x not in serial_numbers]\n",
    "serial_n_to_keep = random.sample(serial_n_to_keep, int(0.05*len(serial_n_to_keep)))\n",
    "serial_n_to_keep = serial_n_to_keep + serial_numbers\n",
    "df = df[df['serial_number'].isin(serial_n_to_keep)]\n",
    "print(\"Dataset shape before after semi random subset:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat_5_normalized 2\n",
      "stat_10_normalized 2\n",
      "stat_184_normalized 255337\n",
      "stat_187_normalized 255337\n",
      "stat_188_normalized 255337\n",
      "stat_196_normalized 356188\n",
      "stat_197_normalized 2\n",
      "stat_198_normalized 2\n",
      "stat_201_normalized 611523\n"
     ]
    }
   ],
   "source": [
    "# count number of nan values in columns\n",
    "for col in df.columns:\n",
    "    na_sum = df[col].isna().sum()\n",
    "    if na_sum>0:\n",
    "        print(col, na_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating variables with timeshift 1, new dataframe shape is (605636, 23)\n",
      "creating variables with timeshift 2, new dataframe shape is (605636, 32)\n",
      "creating variables with timeshift 3, new dataframe shape is (605636, 41)\n",
      "creating variables with timeshift 4, new dataframe shape is (605636, 50)\n",
      "creating variables with timeshift 5, new dataframe shape is (605636, 59)\n",
      "creating variables with timeshift 6, new dataframe shape is (605636, 68)\n",
      "creating variables with timeshift 7, new dataframe shape is (605636, 77)\n",
      "creating variables with timeshift 8, new dataframe shape is (605636, 86)\n",
      "creating variables with timeshift 9, new dataframe shape is (605636, 95)\n",
      "creating variables with timeshift 10, new dataframe shape is (605636, 104)\n",
      "creating variables with timeshift 11, new dataframe shape is (605636, 113)\n",
      "creating variables with timeshift 12, new dataframe shape is (605636, 122)\n",
      "creating variables with timeshift 13, new dataframe shape is (605636, 131)\n",
      "creating variables with timeshift 14, new dataframe shape is (605636, 140)\n"
     ]
    }
   ],
   "source": [
    "# creating features which will collect the stat information from past X days for each day and for each serial_number\n",
    "n_days_shift = 14\n",
    "\n",
    "for i in range(1,n_days_shift +1):\n",
    "    best_indicators_shifted = [\"{}_shift{}\".format(x,i) for x in best_indicators]\n",
    "    df[best_indicators_shifted] = df.groupby(['serial_number'])[best_indicators].shift(i)\n",
    "#     numerical_features_mormalized_shifted = [\"{}_shift{}\".format(x,i) for x in numerical_features_normalized]\n",
    "#     df[numerical_features_mormalized_shifted] = df.groupby(['serial_number'])[numerical_features_normalized].shift(i)\n",
    "    print(\"creating variables with timeshift {}, new dataframe shape is {}\".format(i,df.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse order because rolling window calculates backwards\n",
    "df = df.sort_values(by='date', ascending = False )\n",
    "\n",
    "# create target variable\n",
    "df['Failure_in_following_7d'] = df.groupby('serial_number')['failure'].rolling(window = 7, min_periods = 1).sum().reset_index(0,drop=True)\n",
    "\n",
    "# reverse back to normal\n",
    "df = df.sort_values(by='date', ascending = True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Walk forward time series modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 preparing hyperparameter  dictionary for gridsearch purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing hyperparameter grid for xgboost algorithm\n",
    "xgb_param_grid ={\n",
    "    'nthread': [1],  # use maximum number of threads\n",
    "    'objective': ['binary:logistic'],\n",
    "    \"grow_policy\": [\"lossguide\"],\n",
    "    \"n_jobs\": [-1],\n",
    "    \"num_class\": [1],\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"max_depth\": [5, 6, 13], \n",
    "#     \"min_child_weight\": [2,5],  \n",
    "#     \"colsample_bytree\": [0.7,0.85],\n",
    "#     \"gamma\": [0, 1],\n",
    "    \"subsample\": [0.7,0.9],\n",
    "    'learning_rate': [0.01,  0.1],\n",
    "    'silent': [1],\n",
    "    'seed': [1337],\n",
    "    \"n_estimators\": [8,16,24]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 defining starting period for walk forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation test splits\n",
    "sorted_list_of_dates= list(df.date.unique())\n",
    "sorted_list_of_dates.sort()\n",
    "\n",
    "\n",
    "# # initial_dates = [x for x in sorted_list_of_datesif x <= np.datetime64(\"2009-06-30T00:00:00\")]\n",
    "# # iteration_dates = [x for x in sorted_list_of_datesif x> np.datetime64(\"2009-06-30T00:00:00\")]\n",
    "\n",
    "# initial dates will be used for train data from the beggining \n",
    "initial_dates = [x for x in sorted_list_of_dates if x <= np.datetime64(\"2020-05-22T00:00:00\")]\n",
    "\n",
    "# iteration dates will be iterated over and used as both as train and test sets\n",
    "iteration_dates = [x for x in sorted_list_of_dates if x> np.datetime64(\"2020-05-22T00:00:00\")]\n",
    "\n",
    "# creating an additional helper list which is a shifted version of the iteration dates\n",
    "shifted_dates = iteration_dates.copy()\n",
    "shifted_dates.pop(0)\n",
    "# inserting a dummy value to keep the length of the lists the same\n",
    "shifted_dates.append('dummy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605636, 141)\n",
      "(605636, 187)\n"
     ]
    }
   ],
   "source": [
    "# perform one hot encoding\n",
    "encoder_obj = OneHotEncoder(cols=categorical_features,\n",
    "                                    use_cat_names=True,\n",
    "                                    handle_missing='return_nan',  # 'error' option has a bug?\n",
    "                                    handle_unknown='return_nan')\n",
    "encoder_obj.fit(df)\n",
    "print(df.shape)\n",
    "df = encoder_obj.transform(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_predictor_cols = ['date', 'serial_number','Failure_in_following_7d']\n",
    "categorical_columns = [x for x in df.columns if 'type' in x]\n",
    "numerical_columns = [x for x in df.columns if x not in non_predictor_cols]\n",
    "numerical_columns = [x for x in numerical_columns if x not in categorical_columns]\n",
    "predictor_features = numerical_columns + categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 implementing the walk forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current latest train date 2020-05-23T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-24T00:00:00.000000000\n",
      "best_score in hyperparametrization in step 1 0.8109813573523251\n",
      "best_params in hyperparametrization in step 1 {'objective': 'binary:logistic', 'base_score': 0.5, 'booster': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 13, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 40, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 1337, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'hist', 'validate_parameters': False, 'verbosity': None, 'grow_policy': 'lossguide', 'nthread': 1, 'num_class': 1, 'seed': 1337, 'silent': 1}\n",
      "auc_list [0.8109813573523251]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-24T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-25T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-25T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-26T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-26T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-27T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-27T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-28T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-28T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-29T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-29T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-30T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-30T00:00:00.000000000\n",
      "Current test and/or validation date 2020-05-31T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227]\n",
      "\n",
      "\n",
      "Current latest train date 2020-05-31T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-01T00:00:00.000000000\n",
      "best_score in hyperparametrization in step 9 0.8006811877957253\n",
      "best_params in hyperparametrization in step 9 {'objective': 'binary:logistic', 'base_score': 0.5, 'booster': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 13, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 40, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 1337, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'hist', 'validate_parameters': False, 'verbosity': None, 'grow_policy': 'lossguide', 'nthread': 1, 'num_class': 1, 'seed': 1337, 'silent': 1}\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-01T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-02T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-02T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-03T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-03T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-04T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-04T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-05T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-05T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-06T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-06T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-07T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-07T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-08T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-08T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-09T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-09T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-10T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-10T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-11T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-11T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-12T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current latest train date 2020-06-12T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-13T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985, 0.8001143206411011]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-13T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-14T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985, 0.8001143206411011, 0.8162860506180105]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-14T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-15T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985, 0.8001143206411011, 0.8162860506180105, 0.7525876867105634]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-15T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-16T00:00:00.000000000\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985, 0.8001143206411011, 0.8162860506180105, 0.7525876867105634, 0.7151984946145961]\n",
      "\n",
      "\n",
      "Current latest train date 2020-06-16T00:00:00.000000000\n",
      "Current test and/or validation date 2020-06-17T00:00:00.000000000\n",
      "best_score in hyperparametrization in step 25 0.7311224904494839\n",
      "best_params in hyperparametrization in step 25 {'objective': 'binary:logistic', 'base_score': 0.5, 'booster': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'gamma': 0, 'gpu_id': -1, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 13, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 40, 'n_jobs': -1, 'num_parallel_tree': 1, 'random_state': 1337, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'hist', 'validate_parameters': False, 'verbosity': None, 'grow_policy': 'lossguide', 'nthread': 1, 'num_class': 1, 'seed': 1337, 'silent': 1}\n",
      "auc_list [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985, 0.8001143206411011, 0.8162860506180105, 0.7525876867105634, 0.7151984946145961, 0.7311224904494839]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final prediction date is: 2020-06-17T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "auc_list = []\n",
    "predicted_actuals = []\n",
    "loop_length = len(iteration_dates)\n",
    "\n",
    "# starting the iteration\n",
    "for i,j in zip(iteration_dates, shifted_dates):\n",
    "\n",
    "    # first element of iteration date is also included in the train set\n",
    "    initial_dates.append(i)\n",
    "\n",
    "    \n",
    "    # defining train set\n",
    "    train_set = df[df['date'].isin(initial_dates)]\n",
    "    X_train = train_set[train_set.columns[~train_set.columns.isin(['Failure_in_following_7d'])]]\n",
    "    y_train = train_set['Failure_in_following_7d']\n",
    "\n",
    "    # apply final model on latest data if iteration is not at end of the list (prediction point)\n",
    "    if i == iteration_dates[-7]:\n",
    "        print(\"\\n\\n\\n\\nFinal prediction date is: {}\".format(i))\n",
    "        \n",
    "        final_prediction_data = df[df['date']==i]       \n",
    "        prob_score = model.predict_proba(final_prediction_data[predictor_features])[:,1]\n",
    "        break\n",
    "        \n",
    "    # otherwise, perform walk forward method\n",
    "    else:\n",
    "        print('\\n\\nCurrent latest train date', initial_dates[-1])\n",
    "        print(\"Current test and/or validation date\", j)\n",
    "        \n",
    "        # defining test_validation_set - for hyperparametrization purposes half of the set will be used for validation half for testing\n",
    "        test_validation_set =df[df['date']==j]\n",
    "        \n",
    "        # defining x and y\n",
    "        X = test_validation_set[test_validation_set.columns[~test_validation_set.columns.isin(['Failure_in_following_7d'])]]\n",
    "        y = test_validation_set['Failure_in_following_7d']\n",
    "        \n",
    "        # perform gridsearch again at three different time points, since the training data increases and perhaps optimal hyperparameters change\n",
    "        if counter in [1,9,25]: \n",
    "\n",
    "            # for hyperparametrization we need to have both train and test sdata\n",
    "            X_validation, X_test, y_validation, y_test = train_test_split(X, y,\n",
    "                                                    stratify=y, \n",
    "                                                    test_size=0.6)\n",
    "\n",
    "            # performing hyperparametrization\n",
    "            best_score = 0\n",
    "            best_params = None\n",
    "            len_params = len(ParameterGrid(xgb_param_grid))\n",
    "                \n",
    "            for param,param_counter in zip(ParameterGrid(xgb_param_grid), range(1, len_params+1)):\n",
    "               \n",
    "                # fit the model\n",
    "                model = xgboost.XGBClassifier(**param)\n",
    "                model.fit(X_train[predictor_features],y_train)\n",
    "\n",
    "                # get auc - a common classification metric\n",
    "                predicted_values = model.predict_proba(X_validation[predictor_features])[:,1]\n",
    "                auc = roc_auc_score(y_validation, predicted_values)\n",
    "\n",
    "                # save results of the best score\n",
    "                if auc > best_score:\n",
    "                    best_score = auc\n",
    "                    best_params = model.get_xgb_params()\n",
    "                    \n",
    "                    if param_counter == len_params:\n",
    "                        predicted_actuals.append([predicted_values, y_validation])\n",
    "\n",
    "            auc_list.append(best_score)\n",
    "            print('best_score in hyperparametrization in step {}'.format(counter), best_score)\n",
    "            print('best_params in hyperparametrization in step {}'.format(counter), best_params)\n",
    "        \n",
    "        # otherwise,  use the current date as the test date before adding it to the training in the next loop iteration \n",
    "        else:\n",
    "            model = xgboost.XGBClassifier(**best_params)\n",
    "            model.fit(X_train[predictor_features],y_train)\n",
    "            \n",
    "            # if iteration is at the end of the list (latest date), do not use it as a test set\n",
    "            if counter == loop_length -7:\n",
    "                print(\"Latest date -7 is the latest date used for training. Model will be trained, and latest date will be for prediction\")\n",
    "            \n",
    "            # otherwise, use it as a test set\n",
    "            else:\n",
    "                predicted_values = model.predict_proba(X[predictor_features])[:,1]\n",
    "                auc = roc_auc_score(y, predicted_values)\n",
    "                auc_list.append(auc)\n",
    "                predicted_actuals.append([predicted_values, y])\n",
    "\n",
    "       # changing parameters for next loop iteration\n",
    "        counter+= 1\n",
    "        print(\"auc_list\", auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC across test sets [0.8109813573523251, 0.8175573360776243, 0.8506707237272608, 0.8546902302394904, 0.8517141889357995, 0.8487804154302669, 0.8502090425167349, 0.8418670678735227, 0.8006811877957253, 0.8233986603072704, 0.7930932402360099, 0.7721229278460866, 0.7751091299200947, 0.8071766895035102, 0.9235864416814684, 0.8597573490843575, 0.837850812407681, 0.8734898289333287, 0.8334286133098798, 0.8170932065289985, 0.8001143206411011, 0.8162860506180105, 0.7525876867105634, 0.7151984946145961, 0.7311224904494839]\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC across test sets\", auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average auc 0.8183426997096477\n"
     ]
    }
   ],
   "source": [
    "print(\"Average auc\", np.mean(auc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explanation and reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Disclaimer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset is relatively large and problematic for most personal computers such as mine\n",
    "- Analysis was performed on Amazon virtual instance with 32 GB RAM and 12 cores\n",
    "- Even in this configuration, only a subset of the dataset was used for training adue to RAM constraint and task time limit\n",
    "- Dataset has an intrinsic time component which means the train and test sets need to be carefully designed. No crossvalidation or any type of traditional random splits would be okay in this situation\n",
    "- Because of this, I implemented a so called walk forward method of training a model which allowed me to incrementally increase training set size and ammount of test results\n",
    "- extreme gradient boosting, a state of the art Machine Learning algorithm, was used to create a predictive model\n",
    "- hyperparametrization was performed in multiple steps over a time period\n",
    "- dataset is extremely balanced which makes problem solution very challenging\n",
    "- Amazon instance had problems with the matplot library and for this reason no plots are present in this notebook\n",
    "- only basic feature engineering was done due to time constraints for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC was used as an evaluation metric, since accuracy is pointles for imbalanced datasets. \n",
    "- Average AUC across 25 test sets was 0.81. Perfect model has AUC 1, random model would have AUC 0.5\n",
    "- whether 0.81 or not is a good score, would depend on what the baseline in the industry for these types of models is, and this is not something that I am currently familiar with\n",
    "- For example, in psychological studies AUC higher than 0,7 can in some cases be considered very good, AUC of 0,8 of a model identifying profitable investments in banking could be excellent, while AUC of 0,95  when classifying handwritten digits could be nothing special at all. \n",
    "- it is fair to mention that average AUC of 0.81 is probably too optimistic for two or more reasons:\n",
    "    - AUC across test sets is not stable enough to make any final conclusions\n",
    "    - large ammount of sensors with no failures were dropped due to memory constraints. While this may not always significantly affect the quality of model, there is a chance that this artificial balancing of the dataset made the evaluation overly optimistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Conclusion and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A typical Machine Learning approach was used to demonstrate the effectivenes in solving car sensors failure prediction\n",
    "- Results might be decent at first glance, though somewhat unstable and overly optimistic to certain degree\n",
    "- If this was a real project my recommendation would be:\n",
    "    - first of all, perform preliminary exploratory analysis for interpretation purposes. Idealy, implement a method called Weight of Evidence, which will analyse the effects on the target varible for all features in univariate fashion\n",
    "    - check if any features are very obvious indicators of failure. In some cases model is not needed at all, a good rule based method might be more appropriate\n",
    "\n",
    "    - further explore the walk forward method but:\n",
    "        - use stronger machine, perhaps even a cluster if data is even bigger in reality\n",
    "        - put strong focus on feature engineering\n",
    "        - perform more detailed hypetparametrization\n",
    "    - Explore additional approaches towards solving the problem:\n",
    "        - some other ML algorithms\n",
    "        - anomaly detection methods\n",
    "        - stacking models, e.g. an ML model and a rule based event model on top \n",
    "        - although I am not a fan of dataset balancing in scenarios where data is naturally imbalanced such as here, trying\n",
    "    SMOTE method or something similar might be worth a try\n",
    "    - AUC, although one of the best evaluation metrics across industries, might not be the optimal in a heavily imbalanced scenario. Lift curve might be worth considering if the bussines goal would bo to replace in advance certain number of sensors for which the model gave highest probability of failure. Also, precision-recall, or any similar decision threshold dependant metric could be useful if bussiness can decide on the optimal ratio of true positives vs. false positives "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
